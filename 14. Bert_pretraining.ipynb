{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "twelve-inventory",
   "metadata": {},
   "source": [
    "## 14. BERT Pretraining \n",
    "이 프로젝트에서는 BERT Pretraining 모델을 만들었습니다. 주어진 조건은 다음과 같습니다. \n",
    "* vocab size = 8000\n",
    "* epoch = 10\n",
    "* parameter 수를 1M으로 한다고 했지만 다음과 같은 hyperparameter로 모델을 구성, 모델의 parameter의 수는 3.7M이였다. \n",
    "    * d_model = 256\n",
    "    * n_head = 4\n",
    "    * d_head = 64\n",
    "    * dropout = 0.1\n",
    "    * n_layer = 2\n",
    " \n",
    " \n",
    "#### 결과\n",
    "모델이 NSP에 대한 성능이 MLM에 비해서 좋았습니다. 결과를 시각화 한 것은 제일 마지막에 있는 그래프를 참고 하세요.     \n",
    "    * NSP(next sentence prediction): 다음 문장을 예측하는 기능이고, 모델이 문장들 간의 관계를 파악하게끔 합니다. \n",
    "        * nsp_loss는 매 epoch 마다 줄었고, nsp_accuracy는 매 epoch 마다 상승했습니다. \n",
    "        * 최종 nsp_loss = 0.5947이고 nsp_accuracy는 0.6660 이였습니다. \n",
    "    * MLM(Masked Language Model): 일부token(15%)를 random 하게 masking 해서 모델이 masking한 단어를 알아맞추게 함으로써, 모델이 한 문장 안에서 문맥을 파악하게 훈련합니다.  \n",
    "        * 매 epoch 마다 loss의 감소치는 매우 미미하고 accuracy는 꾸준히 늘었지만 최종 accuracy 크게 높지 않았습니다. \n",
    "        * mlm_loss = 12.2155, mlm_lm_acc = 0.2261"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-contrary",
   "metadata": {},
   "source": [
    "### Tokenizer  \n",
    "SentencePiece 기반의 토크나이저를 준비하는 것으로 BERT pretrain 과정을 시작할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "existing-messenger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-entity",
   "metadata": {},
   "source": [
    "한글 나무위키 코퍼스로부터 32000의 vocab_size를 갖는 sentencepiece 모델을 생성해 보겠습니다. BERT에 사용되는 [MASK], [SEP], [CLS] 등의 주요 특수문자가 vocab에 포함되어야 함에 주의해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "anticipated-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "prefix = 'ko_8000'\n",
    "vocab_size = 8000\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "unauthorized-experiment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/models'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_8000.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-comfort",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "young-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "    # random mask를 위해서 순서를 섞음\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0..1 사이의 확률 값\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]  # mask된 token의 index\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]  # mask된 token의 원래 값\n",
    "\n",
    "    return tokens, mask_idx, mask_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "continued-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "monetary-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for [CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line\n",
    "        current_length += len(doc[i])\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "            # tokens & aegment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "            \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "grand-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "                    if 0 < len(doc):\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "freelance-attempt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2fdebeb907495ab77fc42525dcbf6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_file = os.getenv(\"HOME\") + '/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/bert_pretrain/data/bert_pre_train_8000.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128) # vocab은 위에서 spm.SentencePieceProcessor 로 지정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "formal-charter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918189"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "removable-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "                \n",
    "    # 만약 일반적인 Numpy Array에다 데이터를 로딩한다면 이렇게 되겠지만\n",
    "    # enc_tokens = np.zeros((total, n_seq), np.int32)\n",
    "    # dec_tokens = np.zeros((total, n_seq), np.int32)\n",
    "    # labels_nsp = np.zeros((total,), np.int32)\n",
    "    # labels_mlm = np.zeros((total, n_seq), np.int32)\n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "orange-malta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3a8f87f854485faaafef5277b052ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "protecting-bottle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([   5,   18, 3686,  207, 3714,    4, 3324, 1042,  103, 3610, 3686,\n",
       "         3718,  207, 3714,   37, 3418,  416,  810, 3666, 3625,  131, 3662,\n",
       "            7, 3629,  203,  241, 3602, 1114, 3724,  788,  243,   49, 3632,\n",
       "          796,  663, 1647, 3682, 3682, 3625,  203, 3008, 3625, 3616,   16,\n",
       "         3599,    6,    6,  207, 3714, 3602, 1755, 3630, 3646,  630, 3714,\n",
       "            6,    6,    6,    6,    6,    6, 1369,   10,    6,    6, 1755,\n",
       "         3630,   41, 3644,  830, 3624, 1135,   52, 3599,   13,   81,   87,\n",
       "         1501,    6,   25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636,\n",
       "         3779, 3601,  249, 3725, 1232,   33,   52, 3599,  479, 3652, 3625,\n",
       "          243, 2780,   14, 1509,  168, 3877,  414,  165,    6,    6,    6,\n",
       "            6,    6,  593,   21, 5007,  399, 1927, 3607,  813,   17, 3599,\n",
       "          307,    6,    6,  103, 4313, 4290,    4], dtype=int32),\n",
       " memmap([   5, 3676,  848, 3784, 1931,   58, 3676,  416, 2316, 3619, 3625,\n",
       "         3617, 3744, 4335,   12, 3625, 3616,  175, 3662,    7, 3629,  203,\n",
       "            6,    6,    6,    6,    6,    6,  143, 3625, 3616,  131, 3662,\n",
       "          342, 3629, 3616, 3602,  176,  334,  829, 1115, 3665,    6,    6,\n",
       "         3451, 1633,  375,  671, 1644, 3608,  547, 3423,  765,  815, 3604,\n",
       "            6,    6,    6, 2375, 3608, 3604,  532, 2589, 3599,    4,  307,\n",
       "          323,    6,  321, 3611,  622,  122, 3725, 3620, 3627, 3837, 3608,\n",
       "            6,  176,  268, 4082,   94,  567, 4014, 3617, 7474, 3616, 3830,\n",
       "           66, 3590,  307,  192, 1272,  158, 3788,  353, 3599,  202,  316,\n",
       "         3600,  176,   10,  323,  476, 3663, 1329,  605,  238, 3631, 2470,\n",
       "         3604, 1939,  106, 3627,   13,    6,    6, 1128,   48,    6,    6,\n",
       "          848, 3784, 3833,    8, 3637, 2263,    4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 1,\n",
       " 1,\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,   18, 3686,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "         3565, 3835,  429, 3740, 3628, 3626,    0,    0, 1605, 3599,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0, 2247,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0, 1697, 4290, 3873,\n",
       "         3703, 3683,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,  587,  931,    0,    0,    0,    0], dtype=int32),\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          578, 3652, 3625, 3617, 4148, 3665,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1381, 4148,\n",
       "         3451,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          752, 3608, 3604,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0, 2143,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          347,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,  162,  490,    0,    0,   28, 3599,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-pixel",
   "metadata": {},
   "source": [
    "### BERT 모델 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "polish-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "prepared-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + K.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dutch-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "designing-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "breeding-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bizarre-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: positional embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "transparent-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "extended-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "general-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "posted-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "voluntary-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionalEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "expressed-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "uniform-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fourth-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 2,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 2, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "round-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "outside-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "changing-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "missing-cleveland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "missing-processor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 3695872     enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 3,762,176\n",
      "Trainable params: 3,762,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "executive-leave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 20000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "detected-customs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 413s 205ms/step - loss: 21.0548 - nsp_loss: 0.6685 - mlm_loss: 20.3863 - nsp_acc: 0.5626 - mlm_lm_acc: 0.0974\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.10666, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 410s 205ms/step - loss: 17.8082 - nsp_loss: 0.6265 - mlm_loss: 17.1817 - nsp_acc: 0.6153 - mlm_lm_acc: 0.1272\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.10666 to 0.12931, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 410s 205ms/step - loss: 16.9510 - nsp_loss: 0.6148 - mlm_loss: 16.3362 - nsp_acc: 0.6262 - mlm_lm_acc: 0.1366\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.12931 to 0.13937, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 411s 205ms/step - loss: 15.4440 - nsp_loss: 0.6143 - mlm_loss: 14.8297 - nsp_acc: 0.6270 - mlm_lm_acc: 0.1607\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.13937 to 0.16877, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 410s 205ms/step - loss: 14.0721 - nsp_loss: 0.6102 - mlm_loss: 13.4620 - nsp_acc: 0.6354 - mlm_lm_acc: 0.1902\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.16877 to 0.19350, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 410s 205ms/step - loss: 13.5187 - nsp_loss: 0.6073 - mlm_loss: 12.9114 - nsp_acc: 0.6401 - mlm_lm_acc: 0.2055\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.19350 to 0.20713, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 410s 205ms/step - loss: 13.1907 - nsp_loss: 0.6034 - mlm_loss: 12.5874 - nsp_acc: 0.6476 - mlm_lm_acc: 0.2152\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.20713 to 0.21570, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 411s 205ms/step - loss: 12.9730 - nsp_loss: 0.5994 - mlm_loss: 12.3736 - nsp_acc: 0.6530 - mlm_lm_acc: 0.2209\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.21570 to 0.22115, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 411s 205ms/step - loss: 12.8646 - nsp_loss: 0.5959 - mlm_loss: 12.2686 - nsp_acc: 0.6618 - mlm_lm_acc: 0.2240\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.22115 to 0.22438, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 411s 205ms/step - loss: 12.8102 - nsp_loss: 0.5947 - mlm_loss: 12.2155 - nsp_acc: 0.6660 - mlm_lm_acc: 0.2261\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.22438 to 0.22560, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=[save_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "progressive-supplier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "    \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-intent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
